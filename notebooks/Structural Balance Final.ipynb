{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Balance\n",
    "### Social Network Analysis - Group 9 Project\n",
    "* Miriam Schwebler - 01000421\n",
    "* Isaak Mengesha - 11945608\n",
    "* Sabrina Kuhrn - 00926421\n",
    "* Stefan Hödl - 01452750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:15:45.254093Z",
     "start_time": "2021-01-19T10:15:44.865481Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from networkx import nx\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import matrix_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data set is provided by Der Standard, one of the top Austrian newspapers.\n",
    "In the online Standard people can post comments below articles and up/down vote comments.\n",
    "The data set used in this handson and further in the project part of the course will consider a sample of those articles, comments, and votes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:15:47.657165Z",
     "start_time": "2021-01-19T10:15:45.255686Z"
    }
   },
   "outputs": [],
   "source": [
    "date_cols = [\"PostingCreatedAt\",\"ArticlePublishingDate\"]\n",
    "\n",
    "df1 = pd.read_csv('../data/Postings_01052019_15052019.csv',usecols=[\"ID_CommunityIdentity\", \"ID_Posting\", \"PostingCreatedAt\", \"ArticleTitle\",'ArticleChannel' ,\"ArticleRessortName\",\"ArticlePublishingDate\"],parse_dates=date_cols, sep=';')\n",
    "df2 = pd.read_csv('../data/Postings_16052019_31052019.csv', usecols=[\"ID_CommunityIdentity\", \"ID_Posting\",\"PostingCreatedAt\", \"ArticleTitle\",'ArticleChannel' ,\"ArticleRessortName\",\"ArticlePublishingDate\"], parse_dates=date_cols,sep=';')\n",
    "df  = df1.append(df2, ignore_index=True)\n",
    "df  = df[(df.ArticleChannel == \"Inland\") & (~df.ArticleRessortName.isin([ \"Pensionen\", \"Eurofighter\",\"Off-Topic\"]))]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different entities in the data set: \n",
    "* Users - identified by *ID_CommunityIdentity* (or *UserCommunityName*)\n",
    "* Postings - identified by *ID_Posting*\n",
    "* Articles - identified by *ID_Article*\n",
    "\n",
    "Thus, there are different possibilities to build networks based on voting and posting data. \n",
    "We will concentrate now on the ***votes-to-network***. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:15:53.889967Z",
     "start_time": "2021-01-19T10:15:47.658768Z"
    }
   },
   "outputs": [],
   "source": [
    "date_cols = [\"VoteCreatedAt\",\"UserCreatedAt\"]\n",
    "votes1 = pd.read_csv('../data/Votes_01052019_15052019.csv',parse_dates=date_cols, sep=';')\n",
    "votes2 = pd.read_csv('../data/Votes_16052019_31052019.csv', parse_dates=date_cols,sep=';')\n",
    "votes=votes1.append(votes2, ignore_index=True)\n",
    "\n",
    "PostAndVotes=pd.merge(df,votes,on=\"ID_Posting\")\n",
    "PostAndVotes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:16:11.412703Z",
     "start_time": "2021-01-19T10:15:53.891289Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter off (= 1)\n",
    "PostAndVotes_less=PostAndVotes.groupby('ID_Posting').filter(lambda x : len(x)>1).copy()\n",
    "split_date= datetime.datetime(2019,5,17)\n",
    "\n",
    "PostAndVotes_before = PostAndVotes_less.loc[PostAndVotes_less['PostingCreatedAt'] <= split_date]\n",
    "PostAndVotes_after = PostAndVotes_less.loc[PostAndVotes_less['PostingCreatedAt'] > split_date]\n",
    "\n",
    "#PostAndVotes_after.head()\n",
    "print('Before shape: ' + str(PostAndVotes_before.shape))\n",
    "print('After shape: ' + str(PostAndVotes_after.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line in the table above shows that a user (i.e., *ID_CommunityIdentiy*) posted a comment. Every post has its own uniqe identifier (i.e., *ID_Posting*). If a user votes for a posting then the vote is identified by the *ID_Posting* the voting was for, the *ID_CommunityIdentiy* from the voter. Next, it is also recorded, if the vote was negative or positive. This informtion is saved in  *VoteNegative* and *VotePositive* respectively.  \n",
    "\n",
    "We want to bring the structure above into following format: \n",
    "* source, i.e., the voting user\n",
    "* target, i.e., the post creator\n",
    "* weight, i.e., how often the source voted for the target (postive and negative)\n",
    "\n",
    "In other words, we are aiming for a *weighted edge-list*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:16:11.827199Z",
     "start_time": "2021-01-19T10:16:11.413857Z"
    }
   },
   "outputs": [],
   "source": [
    "edgeListBefore= PostAndVotes_before.groupby([\"ID_CommunityIdentity_x\",\"ID_CommunityIdentity_y\"]).agg({\"VoteNegative\": [(\"votes_neg_count\",\"sum\")], \"VotePositive\":[(\"votes_pos_count\",\"sum\")]})\n",
    "edgeListAfter= PostAndVotes_after.groupby([\"ID_CommunityIdentity_x\",\"ID_CommunityIdentity_y\"]).agg({\"VoteNegative\": [(\"votes_neg_count\",\"sum\")], \"VotePositive\":[(\"votes_pos_count\",\"sum\")]})\n",
    "\n",
    "edgeListBefore.columns=edgeListBefore.columns.droplevel()\n",
    "edgeListAfter.columns=edgeListAfter.columns.droplevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:00:01.585584Z",
     "start_time": "2020-12-06T11:53:07.788Z"
    }
   },
   "source": [
    "### Weight calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:16:11.874855Z",
     "start_time": "2021-01-19T10:16:11.828303Z"
    }
   },
   "outputs": [],
   "source": [
    "## (-1 if any_neg_vote else 1)\n",
    "edgeListBefore[\"weight\"]= np.where(edgeListBefore[\"votes_neg_count\"] > 0, -1, 1) \n",
    "edgeListAfter[\"weight\"]= np.where(edgeListAfter[\"votes_neg_count\"] > 0, -1, 1) \n",
    "\n",
    "## alternative\n",
    "# edgeListBefore[\"weight\"]= (1+edgeListBefore[\"votes_pos_count\"])/(1+edgeListBefore[\"votes_neg_count\"])\n",
    "# edgeListAfter[\"weight\"]=(1+edgeListAfter[\"votes_pos_count\"])/(1+edgeListAfter[\"votes_neg_count\"])\n",
    "\n",
    "edgeListBefore.rename_axis(['source', 'target'], inplace=True)\n",
    "edgeListAfter.rename_axis(['source', 'target'], inplace=True)\n",
    "#edgeListAfter.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:16:15.772851Z",
     "start_time": "2021-01-19T10:16:11.875932Z"
    }
   },
   "outputs": [],
   "source": [
    "## write to file\n",
    "edgesBefore = edgeListBefore.reset_index()\n",
    "edgesAfter = edgeListAfter.reset_index()\n",
    "\n",
    "edgesBefore.to_csv(\"../data/edges_before.csv\", index=False)\n",
    "edgesAfter.to_csv(\"../data/edges_after.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Graph\n",
    "\n",
    "We use the *networkx* library.\n",
    "Since we build a *votes-to-network* we have *source* nodes and *target* nodes. \n",
    "Thus, the network is directed.\n",
    "Therefore, we use *nx.Digraph()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:16:26.607689Z",
     "start_time": "2021-01-19T10:16:15.776845Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edgesAfter, \n",
    "                            source='source', \n",
    "                            target='target', \n",
    "                            edge_attr = 'weight',\n",
    "                            create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract dense subgraph\n",
    "* Create undirected Adjacency Matrix from diGraph \n",
    "\n",
    "* take its third power ($A^3$)\n",
    "\n",
    "* diagonal captures in how many triads a node _might_ be part of\n",
    "\n",
    "* store values in dict, which measure number of possible cycles of length 3 (= triad)\n",
    "\n",
    "* extract subgraph with only nodes which might be part of a triad\n",
    "\n",
    "* thus reduce number of nodes from 20K to 287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T11:40:23.680967Z",
     "start_time": "2020-12-14T11:40:23.632873Z"
    }
   },
   "outputs": [],
   "source": [
    "## use undirected graph and convert to adjacency matrix\n",
    "A = np.abs(nx.to_numpy_matrix(G.to_undirected()))\n",
    "\n",
    "## take matrix to 3rd power\n",
    "A3 = matrix_power(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T10:22:09.625252Z",
     "start_time": "2021-01-19T10:22:08.994415Z"
    }
   },
   "outputs": [],
   "source": [
    "## store in a dictionary (A3di): diagonal & undirected & its a dict!\n",
    "A3di = {}\n",
    "for i in range(len(A3)):\n",
    "    ## store node if potentially part in > 0 triads\n",
    "    if (A3[i,i] > 0):\n",
    "        A3di[i] = A3[i,i] \n",
    "print(len(A3di.keys()))\n",
    "\n",
    "##  Make Sub-dictionary with only highly connected ones\n",
    "A3di_mini = {k:v for k,v in A3di.items() if v > 200}\n",
    "print(len(A3di_mini.keys()))\n",
    "\n",
    "# write after dict to file\n",
    "A3di_after_df = pd.DataFrame.from_dict(A3di, orient='index')\n",
    "A3di_after_df.to_csv(\"../dicts/A3di_after.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for first half of dataset (before Ibiza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T11:46:16.097280Z",
     "start_time": "2020-12-14T11:46:08.846179Z"
    }
   },
   "outputs": [],
   "source": [
    "## construct directed graph, convert to undirected adjacency matrix\n",
    "G_b = nx.from_pandas_edgelist(edgesBefore, \n",
    "                            source='source', \n",
    "                            target='target', \n",
    "                            edge_attr = 'weight',\n",
    "                            create_using=nx.DiGraph())\n",
    "Adj_b = nx.to_numpy_matrix(G_b.to_undirected())\n",
    "\n",
    "## 3rd power\n",
    "A3_b = matrix_power(Adj_b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T11:55:27.553126Z",
     "start_time": "2020-12-14T11:55:27.522946Z"
    }
   },
   "outputs": [],
   "source": [
    "A3di_b = {}\n",
    "for i in range(len(A3_b)):\n",
    "    if (A3_b[i,i] > 0):\n",
    "        A3di_b[i] = A3_b[i,i] \n",
    "len(A3di_b.keys())\n",
    "\n",
    "# write before dict to file\n",
    "A3di_before_df = pd.DataFrame.from_dict(A3di_b, orient='index')\n",
    "A3di_before_df.to_csv(\"../dicts/A3di_before.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the subgraph which should be highly connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T11:55:37.800795Z",
     "start_time": "2020-12-14T11:55:37.780665Z"
    }
   },
   "outputs": [],
   "source": [
    "# read before dictionary from file\n",
    "A3di_before_df = pd.read_csv(\"../dicts/A3di_before.csv\", index_col=0)\n",
    "A3di_b = A3di_before_df['0'].to_dict()\n",
    "# read after dictionary from file\n",
    "A3di_after_df = pd.read_csv(\"../dicts/A3di_after.csv\", index_col=0)\n",
    "A3di = A3di_after_df['0'].to_dict()\n",
    "print(len(A3di.keys()), len(A3di_b.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:05:33.106544Z",
     "start_time": "2020-12-19T14:05:33.100619Z"
    }
   },
   "outputs": [],
   "source": [
    "## possibly filter for value > k\n",
    "A3di_mini = {k:v for k,v in A3di.items() if v > 200}\n",
    "len(A3di_mini)\n",
    "\n",
    "## construct subgraph using dictionary\n",
    "Gmini = G.subgraph(A3di_mini.keys()).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triadic Census Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import nxtriads as nx2 ## import modified functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_double_edge(triad_edge_data):\n",
    "    for i in triad_edge_data:\n",
    "        for j in triad_edge_data:\n",
    "            if (i[0]==j[1] and i[1]==j[0]):\n",
    "                x = i[0]\n",
    "                y = i[1]\n",
    "    return x,y\n",
    "\n",
    "def find_double_edge2(triad_edge_data):\n",
    "    double_edges = []\n",
    "    for i in triad_edge_data:\n",
    "        for j in triad_edge_data:\n",
    "            if (i[0]==j[1] and i[1]==j[0]):\n",
    "                x = i[0]\n",
    "                y = i[1]\n",
    "                double_edges.append((x,y))\n",
    "    return double_edges\n",
    "\n",
    "def Reverse(tuples): \n",
    "    new_tup = tuples[::-1] \n",
    "    return new_tup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triads_by_specific_types(G, my_types):\n",
    "    \"\"\"Returns a list of triads of pre-defined types in a directed graph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : digraph\n",
    "       A NetworkX DiGraph\n",
    "    Returns\n",
    "    -------\n",
    "    tri_by_type : dict\n",
    "       Dictionary with triad types as keys and lists of triads as values.\n",
    "    \"\"\"\n",
    "    all_tri = nx2.all_triads_mod(G) # modified function to skip if len < 3\n",
    "    tri_by_type = defaultdict(list)\n",
    "    for triad in all_tri:\n",
    "        if (nx2.triad_type(triad) in my_types):\n",
    "            name = nx2.triad_type(triad)\n",
    "            tri_by_type[name].append(triad)\n",
    "    return tri_by_type\n",
    "\n",
    "def balance_for_2_semicycles(triad_edge_data):\n",
    "    semicycle_balance = 1\n",
    "    x, y = find_double_edge(triad_edge_data)\n",
    "    positive_set = []\n",
    "    negative_set = []\n",
    "    \n",
    "    # calculating balance/imbalance for both semicycles\n",
    "    for edge in triad_edge_data:\n",
    "        semicycle_balance *= (edge[2])['weight']\n",
    "        \n",
    "    for edge in triad_edge_data:\n",
    "        if ((x == edge[0]) and (y == edge[1])):\n",
    "            semicycle_balance_1 = semicycle_balance / ((edge[2])['weight'])\n",
    "        if ((x == edge[1]) and (y == edge[0])):\n",
    "            semicycle_balance_2 = semicycle_balance / ((edge[2])['weight'])\n",
    "            \n",
    "    # checking for both semicycles if balanced or imbalanced   \n",
    "    if (semicycle_balance_1 > 0):\n",
    "        positive_set.append(semicycle_balance_1)\n",
    "    else:\n",
    "        negative_set.append(semicycle_balance_1)\n",
    "            \n",
    "    if (semicycle_balance_2 > 0):\n",
    "        positive_set.append(semicycle_balance_2)\n",
    "    else:\n",
    "        negative_set.append(semicycle_balance_2)\n",
    "            \n",
    "    triangle_balance = len(positive_set)/(len(positive_set) + len(negative_set))   # step 8 in algorithm             \n",
    "    return triangle_balance\n",
    "\n",
    "def semicycle_extraction(double_edges):\n",
    "    triplets = combinations(double_edges, 3)\n",
    "    triple = []\n",
    "    cycles = []\n",
    "    delete = []\n",
    "    semicycles = []\n",
    "    \n",
    "    for triplet in triplets:\n",
    "        triple.append(triplet)\n",
    "        \n",
    "    for i in range(len(triple)):\n",
    "        if (Reverse((triple[i])[0]) in triple[i]):\n",
    "            delete.append(triple[i])\n",
    "        elif (Reverse((triple[i])[1]) in triple[i]):\n",
    "            delete.append(triple[i])\n",
    "        else:\n",
    "            cycles.append(triple[i])\n",
    "            \n",
    "    nodes = []\n",
    "    for tuples in cycles[0]:\n",
    "        nodes.append(tuples[0])\n",
    "        nodes.append(tuples[1])\n",
    "    unique_nodes = np.unique(nodes)\n",
    "\n",
    "    a = unique_nodes[0]\n",
    "    b = unique_nodes[1]\n",
    "    c = unique_nodes[2]\n",
    "            \n",
    "    for cycle in cycles:\n",
    "        if ((a,b) in cycle and (b,c) in cycle and (c,a) in cycle):\n",
    "            delete.append(cycle)\n",
    "        elif ((b,a) in cycle and (c,b) in cycle and (a,c) in cycle):\n",
    "            delete.append(cycle)\n",
    "        else:\n",
    "            semicycles.append(cycle)\n",
    "\n",
    "    return semicycles\n",
    "\n",
    "def balance_for_triad_type_300(triad_edge_data):\n",
    "    semicycle_balance = 1\n",
    "    double_edges = find_double_edge2(triad_edge_data)\n",
    "    semicycles = semicycle_extraction(double_edges)\n",
    "    \n",
    "    positive_set = []\n",
    "    negative_set = []    \n",
    "\n",
    "    for cycle in semicycles:\n",
    "        semicycle_balance = 1\n",
    "        for i in range(3):\n",
    "            for edge in triad_edge_data:\n",
    "                if (cycle[i] == (edge[0], edge[1])):\n",
    "                    semicycle_balance *= edge[2]['weight']\n",
    "        if semicycle_balance > 0:\n",
    "            positive_set.append(semicycle_balance)\n",
    "        else:\n",
    "            negative_set.append(semicycle_balance)\n",
    "            \n",
    "    triangle_balance = len(positive_set)/(len(positive_set) + len(negative_set))   # step 8 in algorithm             \n",
    "    return triangle_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_ratio(graph):\n",
    "    my_types = {'030T', '120D', '120U', '300'}\n",
    "    triads = triads_by_specific_types(graph, my_types)\n",
    "    \n",
    "    sum_balance_030T = 0\n",
    "    sum_balance_120D = 0\n",
    "    sum_balance_120U = 0\n",
    "    sum_balance_300  = 0\n",
    "    \n",
    "    rel_triads = {k:v for (k,v) in triads.items() if k in my_types}\n",
    "    \n",
    "    for triad_type in my_types:\n",
    "        for i in range(len(triads[triad_type])):\n",
    "            \n",
    "            if (triad_type == '030T'):\n",
    "                edge_data = ((triads['030T'])[i]).edges(data = True)\n",
    "                semicycle_balance_030T = 1\n",
    "                for edge in edge_data:\n",
    "                    semicycle_balance_030T *= (edge[2])['weight']\n",
    "                if (semicycle_balance_030T > 0):\n",
    "                    sum_balance_030T += 1\n",
    "                else:\n",
    "                    sum_balance_030T += 0\n",
    "                \n",
    "            elif (triad_type == '120D'):\n",
    "                edge_data = ((triads['120D'])[i]).edges(data = True)\n",
    "                triangle_balance_120D = balance_for_2_semicycles(edge_data)\n",
    "                \n",
    "                sum_balance_120D += triangle_balance_120D\n",
    "                \n",
    "            elif (triad_type == '120U'):\n",
    "                edge_data = ((triads['120U'])[i]).edges(data = True)\n",
    "                triangle_balance_120U = balance_for_2_semicycles(edge_data)\n",
    "                \n",
    "                sum_balance_120U += triangle_balance_120U\n",
    "                \n",
    "            elif (triad_type == '300'):\n",
    "                edge_data = ((triads['300'])[i]).edges(data = True)\n",
    "                triangle_balance_300 = balance_for_triad_type_300(edge_data)\n",
    "                \n",
    "                sum_balance_300 += triangle_balance_300\n",
    "            \n",
    "    if len(triads['030T']) > 0:\n",
    "        overall_balance_030T = sum_balance_030T / len(triads['030T'])\n",
    "    else: \n",
    "        overall_balance_030T = 0\n",
    "        \n",
    "    if len(triads['120D']) > 0:\n",
    "        overall_balance_120D = sum_balance_120D / len(triads['120D'])\n",
    "    else: \n",
    "        overall_balance_120D = 0\n",
    "        \n",
    "    if len(triads['120U']) > 0:\n",
    "        overall_balance_120U = sum_balance_120U / len(triads['120U'])\n",
    "    else: \n",
    "        overall_balance_120U = 0\n",
    "        \n",
    "    if len(triads['300']) > 0:\n",
    "        overall_balance_300 = sum_balance_300 / len(triads['300'])\n",
    "    else: \n",
    "        overall_balance_300 = 0\n",
    "    \n",
    "    average_overall_balance = (overall_balance_030T + overall_balance_120D + overall_balance_120U + overall_balance_300) / 4\n",
    "\n",
    "    return average_overall_balance, rel_triads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run triadic census algorithm on extracted, dense Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run algorithm on graph after ibiza\n",
    "average_overall_balance, rel_triads = balance_ratio(Gmini)\n",
    "print(average_overall_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run algorithm on graph before ibiza\n",
    "average_overall_balance_before, rel_triads_before = balance_ratio(Gmini)\n",
    "print(average_overall_balance_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting relevant subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified to just extract relevant triads\n",
    "def get_rel_triads(graph):\n",
    "    my_types = {'030T', '120D', '120U', '300'}\n",
    "    triads = triads_by_specific_types(graph, my_types)\n",
    "\n",
    "    return {k:v for (k,v) in triads.items() if k in my_types}\n",
    "\n",
    "rel_triads = get_rel_triads(Gmini)\n",
    "\n",
    "rel = []\n",
    "rels = set()\n",
    "for k,v in rel_triads.items():\n",
    "    for i, tri in enumerate(v):\n",
    "        rel.append(list(tri.nodes))\n",
    "        for idx in list(tri.nodes):\n",
    "            rels.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmicro = Gmini.subgraph(rels).copy()\n",
    "Gmicro_adj = nx.to_numpy_matrix(Gmicro)\n",
    "\n",
    "edges, weights = zip(*nx.get_edge_attributes(Gmicro,'weight').items())\n",
    "nx.draw(Gmicro_adj, node_color='darkblue', edgelist=edges, edge_color=weights, width=2.0, edge_cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: extract Follow-Ignore Relationships \n",
    "* Extract ignore edges for all node-pairs in reduced graph - \n",
    "\n",
    "* only 7 ignore edges remain, so we do not include this in our data \n",
    "\n",
    "* since no timestamp is available, we cannot discern before/after ibiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow = pd.read_csv(('../data/Following_Ignoring_Relationships_01052019_31052019.csv'), sep=';')\n",
    "t1 = follow[follow.ID_CommunityConnectionType == 1]#.iloc[:,:2] # follow\n",
    "t2 = follow[follow.ID_CommunityConnectionType == 2]#.iloc[:,:2] # ignore\n",
    "t2.loc[:,'ID_CommunityConnectionType'] = t2.loc[:,'ID_CommunityConnectionType'].replace(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = []\n",
    "for _, (s,t) in enumerate(zip(t2.ID_CommunityIdentity, t2.ID_CommunityIdentityConnectedTo)):\n",
    "    if (s in A3di.keys()) & (t in A3di.keys()):\n",
    "        ignore.append((s,t))\n",
    "len(ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangle Index & Walk Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import cairo\n",
    "import random\n",
    "from itertools import combinations, groupby\n",
    "from scipy.linalg import expm\n",
    "\n",
    "\n",
    "def gnp_random_connected_graph(n, p):\n",
    "    \"\"\"\n",
    "    Generates a random undirected graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted\n",
    "    \"\"\"\n",
    "    edges = combinations(range(n), 2)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    if p <= 0:\n",
    "        return G\n",
    "    if p >= 1:\n",
    "        return nx.complete_graph(n, create_using=G)\n",
    "    for _, node_edges in groupby(edges, key=lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < p:\n",
    "                G.add_edge(*e, weight=random.choice([1,-1]))\n",
    "    return G\n",
    "\n",
    "\n",
    "nodes = 200\n",
    "probability = 0.1\n",
    "G = gnp_random_connected_graph(nodes,probability)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "nx.draw(G, node_color='lightblue', \n",
    "        with_labels=True, \n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=nx.to_numpy_matrix(G)\n",
    "A3=matrix_power(A,3)\n",
    "A_absolut_3=matrix_power(abs(A),3)\n",
    "#print(A_absolut_3)\n",
    "\n",
    "triangle_index=(np.trace(A3)+np.trace(A_absolut_3))/(2*np.trace(A_absolut_3))\n",
    "print(triangle_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=nx.to_numpy_matrix(G)\n",
    "A_exp_trace=np.trace(expm(A))\n",
    "A_abs_Exp_trace=np.trace(expm(abs(A)))\n",
    "WBM=A_exp_trace/A_abs_Exp_trace #Walk-based measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=nx.to_numpy_matrix(G)\n",
    "H= ig.Graph.Adjacency((abs(A) > 0).tolist())\n",
    "partition =la.find_partition(H, la.ModularityVertexPartition)\n",
    "ig.plot(partition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW: all old = remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgesBefore = pd.read_csv(\"../data/votes_to_comments_before.csv\")\n",
    "# edgesAfter = pd.read_csv(\"../data/votes_to_comments_after.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.from_pandas_edgelist(edgesAfter, \n",
    "#                             source='source', \n",
    "#                             target='target', \n",
    "#                             edge_attr = 'weight',\n",
    "#                             create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UG = G.to_undirected()\n",
    "# #count=0\n",
    "# for node in G:\n",
    "#     for ngbr in nx.neighbors(G, node):\n",
    "#         if node in nx.neighbors(G, ngbr):\n",
    "#             UG.edges[node, ngbr]['weight'] = (np.where( \n",
    "#                 G.edges[node, ngbr]['weight'] + G.edges[ngbr, node]['weight'] >=0,1,-1))\n",
    "#            # if np.sign(G.edges[node, ngbr]['weight'])==np.sign(G.edges[ngbr, node]['weight']):\n",
    "#                                        #       count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = nx.to_numpy_matrix(UG)\n",
    "# A3 = matrix_power(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_absolut_3 = matrix_power(abs(A),3)\n",
    "# triangle_index = (np.trace(A3) + np.trace(A_absolut_3)) / (2*np.trace(A_absolut_3))\n",
    "# triangle_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #edgesBefore = pd.read_csv(\"../data/votes_to_comments_before.csv\")\n",
    "# G_2 = nx.from_pandas_edgelist(edgesBefore, \n",
    "#                             source='source', \n",
    "#                             target='target', \n",
    "#                             edge_attr = 'weight',\n",
    "#                             create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UG_2 = G_2.to_undirected()\n",
    "# for node in G_2:\n",
    "#     for ngbr in nx.neighbors(G_2, node):\n",
    "#         if node in nx.neighbors(G_2, ngbr):\n",
    "#             UG_2.edges[node, ngbr]['weight'] = (np.where( \n",
    "#                 G_2.edges[node, ngbr]['weight'] + G_2.edges[ngbr, node]['weight'] >=0,1,-1))\n",
    "#            # if np.sign(G.edges[node, ngbr]['weight'])==np.sign(G.edges[ngbr, node]['weight']):\n",
    "#                                        #       count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = nx.to_numpy_matrix(UG_2)\n",
    "# A3 = matrix_power(A,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_absolut_3 = matrix_power(abs(A),3)\n",
    "# triangle_index_2 = (np.trace(A3) + np.trace(A_absolut_3)) / (2*np.trace(A_absolut_3))\n",
    "# triangle_index_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
